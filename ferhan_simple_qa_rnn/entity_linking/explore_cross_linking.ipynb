{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import math\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arguments\n",
    "index_entpath = \"../indexes/entity_2M.pkl\"\n",
    "index_reachpath = \"../indexes/reachability_2M.pkl\"\n",
    "index_namespath = \"../indexes/names_2M.pkl\"\n",
    "train_ent_resultpath = \"../entity_detection/query-text/train.txt\"\n",
    "valid_ent_resultpath = \"../entity_detection/query-text/valid.txt\"\n",
    "test_ent_resultpath = \"../entity_detection/query-text/test.txt\"\n",
    "gold_ent_resultpath = \"../entity_detection/gold-query-text/valid.txt\"\n",
    "rel_resultpath = \"../relation_prediction/results/topk-retrieval-valid-hits-3.txt\"\n",
    "outpath = \"./tmp/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def www2fb(in_str):\n",
    "    if in_str.startswith(\"www.freebase.com\"):\n",
    "        return 'fb:%s' % (in_str.split('www.freebase.com/')[-1].replace('/', '.'))\n",
    "    return in_str\n",
    "\n",
    "def get_index(index_path):\n",
    "    print(\"loading index from: {}\".format(index_path))\n",
    "    with open(index_path, 'rb') as f:\n",
    "        index = pickle.load(f)\n",
    "    return index\n",
    "\n",
    "def strip_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_texts(ent_resultpath):\n",
    "    print(\"getting query text...\")\n",
    "    lineids = []\n",
    "    id2query = {}\n",
    "    notfound = 0\n",
    "    with open(ent_resultpath, 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split(\" %%%% \")\n",
    "            try:\n",
    "                lineid = items[0].strip()\n",
    "                queries = items[1:]\n",
    "                # mid = items[2].strip()\n",
    "            except:\n",
    "                # print(\"ERROR: line does not have >2 items  -->  {}\".format(line.strip()))\n",
    "                notfound += 1\n",
    "                continue\n",
    "            # print(\"{}   -   {}\".format(lineid, query))\n",
    "            lineids.append(lineid)\n",
    "            id2query[lineid] = queries\n",
    "    print(\"notfound (empty query text): {}\".format(notfound))\n",
    "    return lineids, id2query\n",
    "\n",
    "def get_relations(rel_resultpath):\n",
    "    print(\"getting relations...\")\n",
    "    lineids = []\n",
    "    id2rels = {}\n",
    "    with open(rel_resultpath, 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split(\" %%%% \")\n",
    "            lineid = items[0].strip()\n",
    "            rel = www2fb(items[1].strip())\n",
    "            label = items[2].strip()\n",
    "            score = items[3].strip()\n",
    "            # print(\"{}   -   {}\".format(lineid, rel))\n",
    "            if lineid in id2rels.keys():\n",
    "                id2rels[lineid].append( (rel, label, score) )\n",
    "            else:\n",
    "                id2rels[lineid] = [(rel, label, score)]\n",
    "                lineids.append(lineid)\n",
    "    return lineids, id2rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    ngrams = zip(*[input_list[i:] for i in range(n)])\n",
    "    return set(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_best_name(question, names_list):\n",
    "    best_score = None\n",
    "    best_name = None\n",
    "    for name in names_list:\n",
    "        score =  fuzz.ratio(name, question)\n",
    "        if best_score == None or score > best_score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "\n",
    "    return best_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting relations...\n",
      "getting query text...\n",
      "notfound (empty query text): 0\n",
      "getting query text...\n",
      "notfound (empty query text): 0\n",
      "getting query text...\n",
      "notfound (empty query text): 0\n"
     ]
    }
   ],
   "source": [
    "rel_lineids, id2rels = get_relations(rel_resultpath)\n",
    "\n",
    "valid_ent_lineids, valid_id2queries = get_query_texts(valid_ent_resultpath)  # ent_lineids may have some examples missing\n",
    "test_ent_lineids, test_id2queries = get_query_texts(test_ent_resultpath)\n",
    "\n",
    "gold_ent_lineids, id2gold_query = get_query_text(gold_ent_resultpath)  # ent_lineids may have some examples missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting questions...\n",
      "108442\n",
      "('fb:m.0f3xg_', 'fb:symbols.namesake.named_after', 'Who was the trump ocean club international hotel and tower named after')\n"
     ]
    }
   ],
   "source": [
    "def get_questions(datapath):\n",
    "    print(\"getting questions...\")\n",
    "    id2question = {}\n",
    "    with open(datapath, 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            lineid = items[0].strip()\n",
    "            sub = items[1].strip()\n",
    "            pred = items[2].strip()\n",
    "            obj = items[3].strip()\n",
    "            question = items[4].strip()\n",
    "            id2question[lineid] = (sub, pred, question)\n",
    "    return id2question\n",
    "\n",
    "datapath = \"../data/SimpleQuestions_v2_modified/all.txt\"\n",
    "id2question = get_questions(datapath)\n",
    "print(len(id2question))\n",
    "print(id2question['valid-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10845\n",
      "21687\n",
      "10845\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_id2queries))\n",
    "print(len(test_id2queries))\n",
    "print(len(id2rels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading index from: ../indexes/entity_2M.pkl\n",
      "loading index from: ../indexes/reachability_2M.pkl\n",
      "loading index from: ../indexes/names_2M.pkl\n"
     ]
    }
   ],
   "source": [
    "num_entities_fbsubset = 200000  # 2M - 1959820 , 5M - 1972702\n",
    "index_ent = get_index(index_entpath)\n",
    "index_reach = get_index(index_reachpath)\n",
    "index_names = get_index(index_namespath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_tf_idf(question, query, cand_ent_name, cand_ent_count, num_entities, index_ent):\n",
    "    query_terms = tokenize_text(cand_ent_name)\n",
    "    doc_tokens = tokenize_text(question)\n",
    "    common_terms = set(query_terms).intersection(set(doc_tokens))\n",
    "\n",
    "    # len_intersection = len(common_terms)\n",
    "    # len_union = len(set(query_terms).union(set(doc_tokens)))\n",
    "    # tf = len_intersection / len_union\n",
    "    tf = math.log10(cand_ent_count + 1)\n",
    "    k1 = 0.5\n",
    "    k2 = 0.5\n",
    "    total_idf = 0\n",
    "    for term in common_terms:\n",
    "        df = len(index_ent[term])\n",
    "        idf = math.log10( (num_entities + k1) / (df + k2) )\n",
    "        total_idf += idf\n",
    "    return tf * total_idf\n",
    "\n",
    "def calc_idf(question, cand_ent_name, index_ent):\n",
    "    query_terms = tokenize_text(cand_ent_name)\n",
    "    doc_tokens = tokenize_text(question)\n",
    "    common_terms = set(query_terms).intersection(set(doc_tokens))\n",
    "    fix_terms = 80000\n",
    "    total_idf = 0\n",
    "    for term in common_terms:\n",
    "        df = len(index_ent[term])\n",
    "        if df > fix_terms:\n",
    "            continue # too common term\n",
    "        idf = math.log10( (fix_terms + 1) / (df + 1) )\n",
    "        total_idf += idf\n",
    "    return total_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['valid-4249',\n",
       " 'valid-2729',\n",
       " 'valid-3909',\n",
       " 'valid-4167',\n",
       " 'valid-3363',\n",
       " 'valid-3239',\n",
       " 'valid-6960',\n",
       " 'valid-4896',\n",
       " 'valid-2431',\n",
       " 'valid-3051']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "sample_lineids = random.sample(rel_lineids, 500)\n",
    "sample_lineids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contains_rel(mid, rels, index_reach):\n",
    "    found = False\n",
    "    for rel in rels:\n",
    "        if rel in index_reach[mid]:\n",
    "            found = True\n",
    "            break\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('linking_lr.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# features_order = [idf, length_name, length_query, length_question, pquer, pques, squer, sques, tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31536798,  0.68463202]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "clf.predict_proba(instance)\n",
    "# clf.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('df_valid_linking.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         idf  length_name  length_query  length_question       lineid  \\\n",
      "0   8.464138            5             7               12  valid-10845   \n",
      "1   8.464138            5             7               12  valid-10845   \n",
      "2   8.464138            8             7               12  valid-10845   \n",
      "3  12.175283            7             7               12  valid-10845   \n",
      "4   8.464138            5             7               12  valid-10845   \n",
      "\n",
      "   name_match_label  pquer  pques         query  squer  sques  tf  true_label  \n",
      "0                 0   0.89   0.89  billy corgan   0.86   0.67   2           0  \n",
      "1                 0   0.89   0.89  billy corgan   0.86   0.67   2           0  \n",
      "2                 0   0.76   0.77  billy corgan   0.75   0.67   2           0  \n",
      "3                 1   1.00   1.00  billy corgan   1.00   0.79   5           1  \n",
      "4                 0   0.89   0.89  billy corgan   0.86   0.67   2           0  \n",
      "0    3991475\n",
      "1      10254\n",
      "Name: true_label, dtype: int64\n",
      "0.9974376075941175\n",
      "0    3505769\n",
      "1     495960\n",
      "Name: name_match_label, dtype: int64\n",
      "0.87606357152121\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: name_match_label, dtype: int64\n",
      "0.958126099993\n",
      "[[ 1.74698843 -3.13496018  0.0327516  -0.07767255  1.62010846  3.12353335]]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df['true_label'].value_counts())\n",
    "print(1-df['true_label'].mean())\n",
    "y = df['name_match_label']\n",
    "print(y.value_counts())\n",
    "print(1-y.mean())\n",
    "print(y.head())\n",
    "# X = df[[\"idf\", \"length_name\", \"length_query\", \"length_question\", \"pquer\", \"pques\", \"squer\", \"sques\", \"tf\"]]\n",
    "fts = [\"idf\", \"length_name\", \"length_query\", \"length_question\", \"sques\", \"tf\"]\n",
    "X = df[fts]\n",
    "#print(X.head())                                                              \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(X, y)      \n",
    "\n",
    "print(lr.score(X, y))\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ent_lineids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5108987e7e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mid2mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mHITS_TOP_ENTITIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_lineids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ent_lineids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_lineids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlineid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ent_lineids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ent_lineids' is not defined"
     ]
    }
   ],
   "source": [
    "id2mids = {}\n",
    "HITS_TOP_ENTITIES = 20\n",
    "sample_lineids = test_ent_lineids[:10]\n",
    "for i, lineid in enumerate(sample_lineids):\n",
    "    if lineid not in test_ent_lineids:\n",
    "        notfound_ent += 1\n",
    "        continue\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"line {}\".format(i))\n",
    "\n",
    "    truth_mid, truth_rel, question = id2question[lineid]\n",
    "    queries = test_id2queries[lineid]\n",
    "#     try:\n",
    "#         queries = id2gold_query[lineid.replace('valid', 'val')]    \n",
    "#     except:\n",
    "#         queries = [ id2question[lineid][2] ]\n",
    "#     rels = [r[0] for r in id2rels[lineid]]\n",
    "    \n",
    "    C = []\n",
    "    C_pruned = []\n",
    "    C_tfidf_pruned = []\n",
    "    \n",
    "    for query in queries:   \n",
    "        query_text = query.lower()  # lowercase the query\n",
    "        query_tokens = tokenize_text(query_text)\n",
    "        N = min(len(query_tokens), 3)\n",
    "        # print(\"lineid: {}, query_text: {}, relation: {}\".format(lineid, query_text, pred_relation))\n",
    "        # print(\"query_tokens: {}\".format(query_tokens))\n",
    "        for n in range(N, 0, -1):\n",
    "            ngrams_set = find_ngrams(query_tokens, n)\n",
    "            # print(\"ngrams_set: {}\".format(ngrams_set))\n",
    "            for ngram_tuple in ngrams_set:\n",
    "                ngram = \" \".join(ngram_tuple)\n",
    "                ngram = strip_accents(ngram)\n",
    "                # unigram stopwords have too many candidates so just skip over\n",
    "                if ngram in stopwords:\n",
    "                    continue\n",
    "                # print(\"ngram: {}\".format(ngram))\n",
    "                try:\n",
    "                    cand_mids = index_ent[ngram]  # search entities\n",
    "                except:\n",
    "                    continue\n",
    "                C.extend(cand_mids)\n",
    "                # print(\"C: {}\".format(C))\n",
    "            if (len(C) > 0):\n",
    "                # print(\"early termination...\")\n",
    "                break\n",
    "        # print(\"C[:5]: {}\".format(C[:5]))\n",
    "\n",
    "        for mid in set(C):\n",
    "            if mid in index_reach.keys():  # PROBLEM: don't know why this may not exist??\n",
    "                count_mid = C.count(mid)  # count number of times mid appeared in C\n",
    "                C_pruned.append((mid, count_mid))\n",
    "#                 if contains_rel(mid, rels, index_reach):\n",
    "#                     count_mid = C.count(mid)  # count number of times mid appeared in C\n",
    "#                     C_pruned.append((mid, count_mid))\n",
    "\n",
    "        for mid, count_mid in C_pruned:\n",
    "            if mid in index_names.keys():\n",
    "                cand_ent_name = pick_best_name(question, index_names[mid])                \n",
    "                length_name = len(tokenize_text(cand_ent_name))\n",
    "                length_question = len(tokenize_text(question))\n",
    "                length_query = len(query_tokens)\n",
    "                tf = count_mid\n",
    "                idf = calc_idf(question, cand_ent_name, index_ent)\n",
    "                sques = fuzz.ratio(cand_ent_name, question)/100.0\n",
    "                squer = fuzz.ratio(cand_ent_name, query_text)/100.0\n",
    "                pques = fuzz.partial_ratio(cand_ent_name, question)/100.0\n",
    "                pquer = fuzz.partial_ratio(cand_ent_name, query_text)/100.0\n",
    "                \n",
    "                instance = np.array([[squer, sques]])\n",
    "                score = lr.predict_proba(instance)[0][1] # get prob of pos-class for example 1\n",
    "\n",
    "                C_tfidf_pruned.append((mid, cand_ent_name, score))\n",
    "        # print(\"C_tfidf_pruned[:10]: {}\".format(C_tfidf_pruned[:10]))\n",
    "\n",
    "    if len(C_tfidf_pruned) == 0:\n",
    "                continue\n",
    "\n",
    "    C_tfidf_pruned.sort(key=lambda t: -t[2])\n",
    "    cand_mids = C_tfidf_pruned[:HITS_TOP_ENTITIES]\n",
    "\n",
    "    id2mids[lineid] = cand_mids\n",
    "\n",
    "\n",
    "# evaluate on sample line ids\n",
    "found = 0\n",
    "notfound = 0\n",
    "notfound_lineids = []\n",
    "\n",
    "for lineid in sample_lineids:\n",
    "    if lineid not in id2mids.keys():\n",
    "        notfound_lineids.append( lineid )\n",
    "        notfound += 1\n",
    "        continue\n",
    "\n",
    "    found_this_example = False\n",
    "    truth_mid, truth_rel, question = id2question[lineid]\n",
    "#     print(id2question[lineid])\n",
    "    for (mid, mid_name, mid_score) in id2mids[lineid]:\n",
    "        if mid == truth_mid:\n",
    "                found_this_example = True\n",
    "                break\n",
    "\n",
    "\n",
    "    if found_this_example:\n",
    "        found += 1\n",
    "    else:\n",
    "        notfound_lineids.append( lineid )\n",
    "        notfound += 1    \n",
    "\n",
    "retrieval = found / (found + notfound) * 100.0\n",
    "print(\"retrieval: {}\\t found: {}\\tnotfound: {}\".format(retrieval, found, notfound))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fb:m.01jp8ww', 'fb:music.album.genre', 'Which genre of album is harder.....faster?')\n",
      "['harder . . . . . faster']\n",
      "['harder ... ..faster', 'harder ... faster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fb:m.04wdfy6', 'dare to dream . . . then do it', 0.0068462255839989265),\n",
       " ('fb:m.04wcmmw', \"the parade 's gone by . . .\", 0.0060080172594988946),\n",
       " ('fb:m.030fpcs', 'unsafe . . at any speed', 0.0036289624043182368),\n",
       " ('fb:m.0f3j_7t', 'now and ... then', 0.0026530460829879382),\n",
       " ('fb:m.01jrzy8',\n",
       "  \"shoot loud , louder. . . i do n't understand\",\n",
       "  0.0022681977772409756),\n",
       " ('fb:m.04j32yl', \"i 'm still here . . . damn it !\", 0.0019640582615284958),\n",
       " ('fb:m.0b_1jj',\n",
       "  'oh my gawd ! ! ! ... the flaming lips',\n",
       "  0.0017672524230145556),\n",
       " ('fb:m.0gjbxhr', 'we who are about to ...', 0.0016897736487917312),\n",
       " ('fb:m.0413g4m', '. . . and some were human', 0.0015698947082201098),\n",
       " ('fb:m.04w6f_j', 'xxx sex . . . tonight', 0.0014125296028915291),\n",
       " ('fb:m.04w2xc7', 'true confessions . . .', 0.001161870239019192),\n",
       " ('fb:m.02q30_d',\n",
       "  'm. choufleuri restera chez lui le . . .',\n",
       "  0.00099311507215096417),\n",
       " ('fb:m.086l0b', 'and having writ ...', 0.00090504244921065988),\n",
       " ('fb:m.0ftbqy6',\n",
       "  'christmas with the . . . new black eagle jazz band',\n",
       "  0.00088780520754784065),\n",
       " ('fb:m.04tw8_r', 'now you see it . . .', 0.00077853611773727319),\n",
       " ('fb:m.04vsg34', 'now you see it . . .', 0.00077853611773727319),\n",
       " ('fb:m.04t3qqq', 'now you see it . . .', 0.00077853611773727319),\n",
       " ('fb:m.03t328', '. . . that thou art mindful of him', 0.00061421772868179385),\n",
       " ('fb:m.04t1p8w',\n",
       "  '. . . if you were there when they signed the constitution',\n",
       "  0.00023110408238833545),\n",
       " ('fb:m.01hg2jc',\n",
       "  'yesterday and today 45th anniversary edition',\n",
       "  0.00020134825676820312)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(id2question['test-1'])\n",
    "print(test_id2queries['test-1'])\n",
    "print(index_names['fb:m.01jp8ww'])\n",
    "id2mids['test-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8adfa1ffb136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mlength_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_ent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mlength_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mlength_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9ef7c6c7af30>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u5/s43mohammed/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "data = defaultdict(list)\n",
    "\n",
    "id2mids = {}\n",
    "HITS_TOP_ENTITIES = 100\n",
    "for i, lineid in enumerate(sample_lineids):\n",
    "    if lineid not in ent_lineids:\n",
    "        notfound_ent += 1\n",
    "        continue\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"line {}\".format(i))\n",
    "\n",
    "    truth_mid, truth_rel, question = id2question[lineid]\n",
    "    queries = id2queries[lineid]    \n",
    "    C = []\n",
    "    C_pruned = []\n",
    "    C_tfidf_pruned = []\n",
    "    \n",
    "    for query in queries:   \n",
    "        query_text = query.lower()  # lowercase the query\n",
    "        query_tokens = tokenize_text(query_text)\n",
    "        N = min(len(query_tokens), 3)\n",
    "        # print(\"lineid: {}, query_text: {}, relation: {}\".format(lineid, query_text, pred_relation))\n",
    "        # print(\"query_tokens: {}\".format(query_tokens))\n",
    "        for n in range(N, 0, -1):\n",
    "            ngrams_set = find_ngrams(query_tokens, n)\n",
    "            # print(\"ngrams_set: {}\".format(ngrams_set))\n",
    "            for ngram_tuple in ngrams_set:\n",
    "                ngram = \" \".join(ngram_tuple)\n",
    "                ngram = strip_accents(ngram)\n",
    "                # unigram stopwords have too many candidates so just skip over\n",
    "                if ngram in stopwords:\n",
    "                    continue\n",
    "                # print(\"ngram: {}\".format(ngram))\n",
    "                try:\n",
    "                    cand_mids = index_ent[ngram]  # search entities\n",
    "                except:\n",
    "                    continue\n",
    "                C.extend(cand_mids)\n",
    "                # print(\"C: {}\".format(C))\n",
    "            if (len(C) > 0):\n",
    "                # print(\"early termination...\")\n",
    "                break\n",
    "        # print(\"C[:5]: {}\".format(C[:5]))\n",
    "\n",
    "        for mid in set(C):\n",
    "            if mid in index_reach.keys():  # PROBLEM: don't know why this may not exist??\n",
    "                count_mid = C.count(mid)  # count number of times mid appeared in C\n",
    "                C_pruned.append((mid, count_mid))\n",
    "\n",
    "        for mid, count_mid in C_pruned:\n",
    "            if mid in index_names.keys():\n",
    "                cand_ent_name = pick_best_name(question, index_names[mid])\n",
    "                try:\n",
    "                    truth_name = pick_best_name(question, index_names[truth_mid])\n",
    "                except:\n",
    "                    continue\n",
    "                if cand_ent_name == truth_name:  # if name is correct, we are good\n",
    "                    data['label'].append(1)\n",
    "                else:\n",
    "                    data['label'].append(0)\n",
    "\n",
    "    #             if mid == truth_mid:\n",
    "    #                 data['label'].append(1)\n",
    "    #             else:\n",
    "    #                 data['label'].append(0)\n",
    "                \n",
    "                length_name = len(tokenize_text(cand_ent_name))\n",
    "                length_question = len(tokenize_text(question))\n",
    "                length_query = len(query_tokens)\n",
    "                tf = count_mid\n",
    "                idf = calc_idf(question, cand_ent_name, index_ent)\n",
    "                sques = fuzz.ratio(cand_ent_name, question)/100.0\n",
    "                squer = fuzz.ratio(cand_ent_name, query_text)/100.0\n",
    "                pques = fuzz.partial_ratio(cand_ent_name, question)/100.0\n",
    "                pquer = fuzz.partial_ratio(cand_ent_name, query_text)/100.0            \n",
    "            \n",
    "                data['length_name'].append(length_name)\n",
    "                data['length_question'].append(length_question)\n",
    "                data['length_query'].append(length_query)\n",
    "                data['tf'].append(tf)\n",
    "                data['idf'].append(idf)\n",
    "                data['sques'].append(sques)\n",
    "                data['squer'].append(squer)\n",
    "                data['pques'].append(pques)\n",
    "                data['pquer'].append()           \n",
    "\n",
    "                C_tfidf_pruned.append((mid, cand_ent_name, data))\n",
    "        # print(\"C_tfidf_pruned[:10]: {}\".format(C_tfidf_pruned[:10]))\n",
    "\n",
    "    if len(C_tfidf_pruned) == 0:\n",
    "        continue\n",
    "        \n",
    "    id2mids[lineid] = C_tfidf_pruned\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8822111021805077"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59353 / (59353 + 8002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    34026\n",
      "1     4543\n",
      "Name: label, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "        idf  length_name  length_query  length_question  pquer  pques  squer  \\\n",
      "0  3.811336            3             4                7   1.00    1.0   0.93   \n",
      "1  3.811336            3             4                7   1.00    1.0   0.93   \n",
      "2  4.198213            4             4                7   1.00    1.0   1.00   \n",
      "3  3.344607           13             4                7   0.88    0.6   0.36   \n",
      "4  3.811336            3             4                7   1.00    1.0   0.93   \n",
      "\n",
      "   sques  tf  \n",
      "0   0.64   1  \n",
      "1   0.64   1  \n",
      "2   0.70   2  \n",
      "3   0.41   1  \n",
      "4   0.64   1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "y = df['label']\n",
    "print(y.head()) \n",
    "X = df.drop('label', axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create LR data\n",
    "# data = []\n",
    "# for lineid in sample_lineids:\n",
    "#     if lineid not in id2mids.keys():\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "#     found_this_example = False\n",
    "#     truth_mid, truth_rel, question = id2question[lineid]\n",
    "# #     print(id2question[lineid])\n",
    "#     for (mid, mid_name, features) in id2mids[lineid]:\n",
    "#         row = [length, tfidf, fuzzy]\n",
    "#         try:\n",
    "#             truth_name = pick_best_name(question, index_names[truth_mid])\n",
    "#         except:\n",
    "#             continue\n",
    "# #         print(mid_name, truth_name)\n",
    "#         if mid_name == truth_name:  # if name is correct, we are good\n",
    "#             found_this_example = True\n",
    "#             row.append(1) # pos example\n",
    "#         else:\n",
    "#             row.append(0) # neg example\n",
    "#         data.append(row)\n",
    "        \n",
    "# df = pd.DataFrame(data)\n",
    "# df.columns = ['length', 'tfidf', 'fuzzy', 'label']\n",
    "# df.dropna(how='all')    #to drop if all values in the row are nan\n",
    "# print(df.describe())\n",
    "# print(df.head())\n",
    "\n",
    "# X = df[['length', 'tfidf', 'fuzzy']]\n",
    "# print(X.head())\n",
    "# y = df['label']\n",
    "# print(y.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976950400581\n",
      "[[ 1.51357524 -2.08075315  0.36106511 -0.3810684  -0.13058176  1.32223429\n",
      "   1.62022576  0.06219903 -0.13805733]]\n",
      "[-0.36577348]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(X, y)\n",
    "\n",
    "print(lr.score(X, y))\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4493"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_ent['mark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy_intersection(tokens, text):\n",
    "    common = []\n",
    "    for tok in tokens:\n",
    "        match_score = fuzz.partial_ratio(tok, text) / 100.0\n",
    "        if match_score > 0.80:\n",
    "            common.append( (tok, match_score) )\n",
    "    return common\n",
    "\n",
    "def custom_match(question, query_text, cand_ent_name, count_mid, num_entities_fbsubset, index_ent):\n",
    "    query_terms = tokenize_text(cand_ent_name)\n",
    "    common_terms = fuzzy_intersection(set(query_terms), question)\n",
    "\n",
    "    # len_intersection = len(common_terms)\n",
    "    # len_union = len(set(query_terms).union(set(doc_tokens)))\n",
    "    # tf = len_intersection / len_union\n",
    "    tf = math.log10(count_mid + 1)\n",
    "    k1 = 0.5\n",
    "    k2 = 0.5\n",
    "    total_idf = 0\n",
    "    for (term, fuzzy_score) in common_terms:\n",
    "        df = len(index_ent[term])\n",
    "        idf = fuzzy_score * math.log10( (num_entities_fbsubset + k1) / (df + k2) )\n",
    "        total_idf += idf\n",
    "    return tf*total_idf\n",
    "\n",
    "def custom_weights(question, query_text, cand_ent_name, count_mid, num_entities_fbsubset, index_ent):\n",
    "    x1 = calc_tf_idf(question, query_text, cand_ent_name, count_mid, num_entities_fbsubset, index_ent)\n",
    "    x2 = fuzz.partial_ratio(cand_ent_name, question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: simple, text: question, hits: 20\n",
      "retrieval: 84.0\t found: 168\tnotfound: 32\n",
      "----------------------------------------\n",
      "sim: partial, text: question, hits: 20\n",
      "retrieval: 81.0\t found: 162\tnotfound: 38\n",
      "----------------------------------------\n",
      "sim: simple, text: query, hits: 20\n",
      "retrieval: 89.0\t found: 178\tnotfound: 22\n",
      "----------------------------------------\n",
      "sim: partial, text: query, hits: 20\n",
      "retrieval: 81.0\t found: 162\tnotfound: 38\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "id2mids = {}\n",
    "HITS_TOP_ENTITIES = 20\n",
    "sims = [\"simple\", \"partial\"]\n",
    "texts = [\"question\", \"query\"]\n",
    "for USE_TEXT in texts:\n",
    "    for SIM in sims:\n",
    "        print(\"sim: {}, text: {}, hits: {}\".format(SIM, USE_TEXT, HITS_TOP_ENTITIES))\n",
    "        if i % 100 == 0:\n",
    "            print(\"line {}\".format(i))\n",
    "        for i, lineid in enumerate(sample_lineids):\n",
    "            if lineid not in ent_lineids:\n",
    "                notfound_ent += 1\n",
    "                continue\n",
    "\n",
    "            truth_mid, truth_rel, question = id2question[lineid]\n",
    "            try:\n",
    "                query_text = id2gold_query[lineid.replace('valid', 'val')].lower()  # lowercase the query\n",
    "            except:\n",
    "                query_text = question.lower()\n",
    "            query_tokens = tokenize_text(query_text)\n",
    "\n",
    "            # print(\"lineid: {}, query_text: {}, relation: {}\".format(lineid, query_text, pred_relation))\n",
    "            # print(\"query_tokens: {}\".format(query_tokens))\n",
    "\n",
    "            N = min(len(query_tokens), 3)\n",
    "            C = []  # candidate entities\n",
    "            for n in range(N, 0, -1):\n",
    "                ngrams_set = find_ngrams(query_tokens, n)\n",
    "                # print(\"ngrams_set: {}\".format(ngrams_set))\n",
    "                for ngram_tuple in ngrams_set:\n",
    "                    ngram = \" \".join(ngram_tuple)\n",
    "                    ngram = strip_accents(ngram)\n",
    "                    # unigram stopwords have too many candidates so just skip over\n",
    "                    if ngram in stopwords:\n",
    "                        continue\n",
    "                    # print(\"ngram: {}\".format(ngram))\n",
    "                    try:\n",
    "                        cand_mids = index_ent[ngram]  # search entities\n",
    "                    except:\n",
    "                        continue\n",
    "                    C.extend(cand_mids)\n",
    "                    # print(\"C: {}\".format(C))\n",
    "                if (len(C) > 0):\n",
    "                    # print(\"early termination...\")\n",
    "                    break\n",
    "            # print(\"C[:5]: {}\".format(C[:5]))\n",
    "\n",
    "            # relation correction\n",
    "            C_pruned = []\n",
    "            for mid in set(C):\n",
    "                if mid in index_reach.keys():  # PROBLEM: don't know why this may not exist??\n",
    "                    count_mid = C.count(mid)  # count number of times mid appeared in C\n",
    "                    C_pruned.append((mid, count_mid))\n",
    "\n",
    "            C_tfidf_pruned = []\n",
    "            for mid, count_mid in C_pruned:\n",
    "                if mid in index_names.keys():\n",
    "                    cand_ent_name = pick_best_name(question, index_names[mid])\n",
    "                    if  USE_TEXT == \"question\":\n",
    "                        text = question\n",
    "                    else:\n",
    "                        text = query_text\n",
    "\n",
    "                    if SIM == \"custom\":\n",
    "                        score = 0.8 * fuzz.token_set_ratio(cand_ent_name, question) + fuzzy_match_score(cand_ent_name, question)\n",
    "                    elif SIM == \"simple\":\n",
    "                        score = fuzz.ratio(cand_ent_name, text) / 100.0\n",
    "                    elif SIM == \"partial\":\n",
    "                        simple_question = fuzz.partial_ratio(cand_ent_name, text) / 100.0\n",
    "                    else:\n",
    "                        score = calc_tf_idf(question, query_text, cand_ent_name, count_mid, num_entities_fbsubset, index_ent)\n",
    "                    C_tfidf_pruned.append((mid, cand_ent_name, score))\n",
    "            # print(\"C_tfidf_pruned[:10]: {}\".format(C_tfidf_pruned[:10]))\n",
    "\n",
    "            if len(C_tfidf_pruned) == 0:\n",
    "                continue\n",
    "\n",
    "            C_tfidf_pruned.sort(key=lambda t: -t[2])\n",
    "            cand_mids = C_tfidf_pruned[:HITS_TOP_ENTITIES]\n",
    "\n",
    "            id2mids[lineid] = cand_mids\n",
    "\n",
    "\n",
    "        # evaluate on sample line ids\n",
    "        found = 0\n",
    "        notfound = 0\n",
    "        notfound_lineids = []\n",
    "\n",
    "        for lineid in sample_lineids:\n",
    "            if lineid not in id2mids.keys():\n",
    "                notfound_lineids.append( lineid )\n",
    "                notfound += 1\n",
    "                continue\n",
    "\n",
    "            found_this_example = False\n",
    "            truth_mid, truth_rel, question = id2question[lineid]\n",
    "        #     print(id2question[lineid])\n",
    "            for (mid, mid_name, mid_score) in id2mids[lineid]:\n",
    "                if mid == truth_mid:\n",
    "                        found_this_example = True\n",
    "                        break\n",
    "\n",
    "\n",
    "            if found_this_example:\n",
    "                found += 1\n",
    "            else:\n",
    "                notfound_lineids.append( lineid )\n",
    "                notfound += 1    \n",
    "\n",
    "        retrieval = found / (found + notfound) * 100.0\n",
    "        print(\"retrieval: {}\\t found: {}\\tnotfound: {}\".format(retrieval, found, notfound))\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid-9773 Name one of the townships in indiana\n",
      "gold entity id: fb:m.03v1s\n",
      "names: ['in', 'ind', 'us-in', 'indiana', 'hoosier state']\n",
      "query text: name one of the townships in indiana\n",
      "MIDS:\n",
      "('fb:m.05zms17', 'an army at dawn : the war in north africa 1942-1943', 0.259241157950361, 0.5)\n",
      "----------------------------------------\n",
      "valid-3981 what's out of eden about\n",
      "gold entity id: fb:m.04t4n1z\n",
      "names: ['out of eden']\n",
      "query text: eden\n",
      "MIDS:\n",
      "('fb:m.015d6d', 'east of eden', 0.8968554509168298, 1.0)\n",
      "----------------------------------------\n",
      "valid-5775 which release was lemon on\n",
      "gold entity id: fb:m.0dxyd5l\n",
      "names: ['lemon']\n",
      "query text: lemon\n",
      "MIDS:\n",
      "('fb:m.0cs3sg3', 'life of lemon', 0.8289980641994564, 1.0)\n",
      "----------------------------------------\n",
      "valid-4123 what is the formulation of clear defense hand sanitizing wipes\n",
      "gold entity id: fb:m.0hqtqnf\n",
      "names: ['alcohol 3.32 cloth']\n",
      "query text: defense hand sanitizing wipes\n",
      "MIDS:\n",
      "('fb:m.0hqtf4n', 'advanced hand sanitizing wipes 0.62/0.003 swab', 3.643475945022978, 0.86)\n",
      "----------------------------------------\n",
      "valid-5197 Which country is the film paradise from\n",
      "gold entity id: fb:m.0mxq5rn\n",
      "names: ['paradise']\n",
      "query text: paradise\n",
      "MIDS:\n",
      "('fb:m.0y4hy9c', 'hell in paradise', 0.7349925935186626, 1.0)\n",
      "----------------------------------------\n",
      "valid-10511 what album is remember me from\n",
      "gold entity id: fb:m.0mmqpgs\n",
      "names: ['remember me']\n",
      "query text: remember me\n",
      "MIDS:\n",
      "('fb:m.0n7l4l', 'remember me ( cavern 3 remix )', 1.0255301397716865, 1.0)\n",
      "----------------------------------------\n",
      "valid-125 What's a science fiction movie featuring godzilla\n",
      "gold entity id: fb:m.06n90\n",
      "names: ['science-fiction', 'sf', 'science fiction', 'science fiction on television', 'science fiction novel', 'sci-fi']\n",
      "query text: science fiction\n",
      "MIDS:\n",
      "('fb:m.05q11by', 'science fiction', 1.3285535997935463, 1.0)\n",
      "----------------------------------------\n",
      "valid-9207 who on earth directed texas\n",
      "gold entity id: fb:m.0bxzpxv\n",
      "names: ['texas']\n",
      "query text: texas\n",
      "MIDS:\n",
      "('fb:m.013m3d', 'carrolton , texas', 0.5392121306536158, 1.0)\n",
      "----------------------------------------\n",
      "valid-6545 what is a gameplay mode featured on richard scarrys busytown\n",
      "gold entity id: fb:m.0cy3lz\n",
      "names: [\"richard scarry 's busytown\"]\n",
      "query text: richard scarrys busytown\n",
      "MIDS:\n",
      "('fb:m.01q408k', 'wolfsdorf , richard', 0.44252181165443194, 0.26)\n",
      "----------------------------------------\n",
      "valid-2199 what is a track off the cubism record\n",
      "gold entity id: fb:m.0mxhng2\n",
      "names: ['cubism']\n",
      "query text: cubism\n",
      "MIDS:\n",
      "('fb:m.0c7hk67', 'cubism', 1.0938160082697637, 1.0)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for id in random.sample(notfound_lineids, 10):\n",
    "    ent, rel, ques = id2question[id]\n",
    "    print(id, ques)\n",
    "    try:\n",
    "        print(\"gold entity id: {}\\nnames: {}\".format(ent, index_names[ent]))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"query text: {}\".format(id2query[id].lower()))\n",
    "    print(\"MIDS:\")\n",
    "    try:\n",
    "        mids = id2mids[id]\n",
    "    except:\n",
    "        continue\n",
    "    for mid in mids:\n",
    "        print(mid)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
